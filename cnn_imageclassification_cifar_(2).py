# -*- coding: utf-8 -*-
"""CNN_ImageClassification_CIFAR (2).ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1W6kJBBgyvKdrB1GoBLV4fX9xwX9v0IMg
"""

from __future__ import absolute_import, division, print_function

import numpy as np
import tensorflow as tf
from keras.datasets import cifar10

# %matplotlib inline

import matplotlib
import matplotlib.pyplot as plt

print(tf.__version__)
print(np.__version__)
print(matplotlib.__version__)

(features, labels), (X_test_orig, Y_test_orig) = cifar10.load_data()
features = features/255 - 0.5
labels = labels.flatten()

test_images = X_test_orig/255 - 0.5
test_labels = Y_test_orig.flatten()

features[0].shape

labels = labels.flatten()
labels

height = 32
width = 32
channels = 3
n_inputs = height * width

tf.reset_default_graph()

def initialize_parameters():
    height = 32
    width = 32
    channels = 3
    n_inputs = height * width
    X = tf.placeholder(tf.float32, shape=[None,  height, width, channels], name="X")
    dropout_rate = 0.3

    training = tf.placeholder_with_default(False, shape=(), name='training')
    X_drop = tf.layers.dropout(X, dropout_rate, training=training)
    y = tf.placeholder(tf.int32, shape=[None], name="y")
    return X_drop, y, training



def get_next_batch(features, labels, train_size, batch_index, batch_size):
    training_images = features[:train_size,:,:]
    training_labels = labels[:train_size]
    
    start_index = batch_index * batch_size
    end_index = start_index + batch_size

    return features[start_index:end_index,:,:], labels[start_index:end_index]

def forward_propagation(X):
  
    # Convolutional Layer #1
    conv1 = tf.layers.conv2d(
      inputs=X,
      filters=32,
      kernel_size=[5, 5],
      padding="valid",
      activation=tf.nn.relu)

    conv2 = tf.layers.conv2d(conv1, filters=64, 
                       kernel_size=3,
                       strides=2, padding="valid",
                       activation=tf.nn.relu, name="conv2")
    pool3 = tf.nn.max_pool(conv2,
                     ksize=[1, 2, 2, 1],
                     strides=[1, 2, 2, 1],
                     padding="VALID")
    conv4 = tf.layers.conv2d(pool3, filters=128, 
                       kernel_size=4,
                       strides=3, padding="SAME",
                       activation=tf.nn.relu, name="conv4")

    pool5 = tf.nn.max_pool(conv4,
                     ksize=[1, 2, 2, 1],
                     strides=[1, 1, 1, 1],
                     padding="VALID")

    pool5_flat = tf.contrib.layers.flatten(pool5)

    fullyconn1 = tf.layers.dense(pool5_flat, 128,
                           activation=tf.nn.relu, name="fc1")

    fullyconn2 = tf.layers.dense(fullyconn1, 64,
                           activation=tf.nn.relu, name="fc2")
    logits = tf.layers.dense(fullyconn2, 10, name="output")
    return logits

tf.reset_default_graph()

n_epochs = 10
batch_size = 128
X, y, training = initialize_parameters()
logits = forward_propagation(X)
xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits,
                                                      labels=y)
loss = tf.reduce_mean(xentropy)
optimizer = tf.train.AdamOptimizer()
training_op = optimizer.minimize(loss)


init = tf.global_variables_initializer()
with tf.Session() as sess:
    sess.run(init)
    for epoch in range(n_epochs):
#         batch_index = 0
        # Add this in when we want to run the training on all batches in CIFAR-10
        batch_index = 0

        train_size = int(len(features))

        for iteration in range(train_size // batch_size):
            X_batch, y_batch = get_next_batch(features, 
                                                                        labels, 
                                                                        train_size, 
                                                                        batch_index,
                                                                        batch_size)
            batch_index += 1

            sess.run(training_op, feed_dict={X: X_batch, y: y_batch, training: True})
            correct = tf.nn.in_top_k(logits, y, 1)
            sess.run(training_op, feed_dict={X: X_batch, y: y_batch, training: True})
        
            accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))
            model_saver = tf.train.Saver()
        acc_train = accuracy.eval(feed_dict={X: X_batch, y: y_batch})
        acc_test = accuracy.eval(feed_dict={X: test_images, y: test_labels})
        print(epoch, "Train accuracy:", acc_train, "Test accuracy:", acc_test)
        print ("Train Accuracy:", round(acc_train, 2)*100,"%",  end="\t")
        print ("Test Accuracy:", round(acc_test, 2)*100,"%", end="\n")
        import os
    if not os.path.exists("ADL"):
        os.makedirs("ADL")
    save_path = model_saver.save(sess, "ADL/cifar_model")

import tensorflow as tf
from tensorflow.python.tools import inspect_checkpoint as chkp

tf.reset_default_graph()
with tf.Session() as sess:
    # Restore variables from disk.
    #X, y, training = initialize_parameters()
    saver = tf.train.import_meta_graph('ADL/cifar_model.meta')
    saver.restore(sess,"ADL/cifar_model")
    print("Model restored.")
    sess.run(accuracy, feed_dict={X: test_images, y: test_labels})
    #print(predicition)



logits





